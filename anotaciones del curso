// CARGA DE DATOS

** pedir informacion del dataset -> delimitadores, formato de los campos nuericos (separador decimal)
** Repositorio https://github.com/joanby/python-ml-course
Investigar como funciona github desktop

** por si no autocompleta jupyter agregar en la primer linea -> %config IPCompleter.greedy=True 

PANDAS package para carga de datasets
	-> cmd df.columns.values para listar las columnas del dataset
	
** Si la descarga de los datos se hace desde la web de forma manual con urllib3,hay que decodificar con "decode('utf-8')" y luego transformar en dataframe (pd.Dataframe(Array de datos))

// DATA CLEANING

** hacer respaldo de los dataset antes de empezar
** con la fn shape de pandas saco la dimension del dataset
** info del estadistico basico -> df.describe()
** verificar si hay campos nulos -> y de haber hay que tratarlos para evitar la dispersion

La fn ravel() arma un array con la filas de los datos

pd.isnull(df['nombre del atributo']).values.ravel().sum() -> esta bueno para para controlar si hay datos nulos

pd.dropna() -> fn para limpiar los nan del df , borra por con coordenadas donde (axis = 0) elimina fila, axis = 1 elimina columna

** Importante es tratar de inferir aquellos datos que no se tienen antes de borrar
	> dependiendo del tipo de dato se podria llenar completar con haciendo un data.fillna(0)
	> Es SUMAMENTE PREFERIBLE sacar el valor promedio del dato faltante usando data.fillna("nombre del dato").mean()
	> Se puede completar el dato tambien con el valor mas cercano al dato anterior data.fillna(method="backfill") o la valor mas cercano al posterior data.fillna(method="ffill") 	 
	
** Variables dummy
	Es una variable ficticia es aquella que toma solo el valor 0 o 1 para indicar la ausencia o presencia de algún efecto categórico que se puede esperar que cambie el resultado como por ejemplo el sexo 
	> Para hacer una variable dummy se llama fn de pandas -> dummy_sex = pd.get_dummies(df['sex'],prefix='sex') 
	> Luego el eliminar aquella columna que genero los dummies con fn pd.drop("nombre del dato") para luego agregar al df las variable dummies
	> Con fn concat -> pd.concat((df, dummy_sex ), axis = 1 )

//PLOTS y VISUALIZACION DE LOS DATOS

	import matplotlib.pyplot  

** Tener en cuenta
	> %matplotlib inline  -> genera el plot dentro del div del notebook
	> savefig(ruta) -> podes guardar tanto local como remoto el plot para ser exportado

*** SCATTER PLOT (genera grafico de nube)
	> Es muy util para ayudar a ver la correlacion entre las variables
	> data.plot(kind='scatter',x='nombre de variable',y='nombre de variable')

plt.subplots(2,2, sharex=True,sharey=True) -> permite agrupar varios plots en un solo grafico los seteos de share es para definir si compaten tanto el eje X como el Y

data.plot(kind='scatter',x='nombre de variable',y='nombre de variable',ax=axs[0][0]) -> se definen la posicion dentro del grafico gral
data.plot(kind='scatter',x='nombre de variable',y='nombre de variable',ax=axs[0][1]) 
data.plot(kind='scatter',x='nombre de variable',y='nombre de variable',ax=axs[1][0])
data.plot(kind='scatter',x='nombre de variable',y='nombre de variable',ax=axs[1][1])
	

	por ejemplo

		figure , axs = plt.subplots(2,2, sharex=True,sharey=True)
		data.plot(kind='scatter',x='Column Name',y='Day Charge',ax=axs[0][0])
		data.plot(kind='scatter',x='Column Name2',y='Night Charge',ax=axs[0][1])
		data.plot(kind='scatter',x='Day Calls',y='Day Charge',ax=axs[1][0])
		data.plot(kind='scatter',x='Night Calls',y='Night Charge',ax=axs[1][1])


// HISTOGRAMA DE FRECUENCIA 
** Son sumamente utiles para ver las distribuciones de variables numericas
	
	por ejemplo

		plt.hist(data['Day Calls'],bins =20)  ->donde bins es la cantidad de divisiones del grafico se la puede pasar un array 
		plt.xlabel('Nro de llamadas')
		plt.ylabel('Frecuencia')
		plt.title('Histograma del nro de llamadas al dia')

** Para calcular la cantidad de divisiones del grafico se realiza aplicando la regla de Sturges ( 1 + log2 (dimension de la muestra))

// BOXPLOT

** Muestra en forma de caja como se disribuyen los datos graficamente, mostrando los 3 cuartiles y los bigotes tanto limite superior como limite inferior en la distribucion de los datos
	

	por ejemplo

		plt.boxplot(data['Day Calls'])
		plt.ylabel('Numero de llamadas diarias')
		plt.title('Boxplot de las llamadas')


// DATA WRANGLING

	Ejemplos de filtrado se pueden hacer inline

		#filter > 150
		data1 = data[data['Column Name']>150]

		#filter state =NY
		data1 = data[data['State']== 'NY']

		#filter state =NY & Column Name > 300
		data1 = data[(data['State']== 'NY')&(data['Column Name']>=300)]

	Ejemplo de acortar dataset 

		subset_first_50 = data[['Column Name','Column Name2','Column Name3']][:50]  #columnas / filas
		subset_first_50.shape

		** otra forma con fn iloc -> las primeras coordenadas son las filas , columnas se puede pasar el index o el nombre de las columnas
	 
		ejemplo        eje x    eje y
			data.iloc[[1,3,5],[2,5,7]]

				Area Code	VMail Plan	Day Mins
			1	415			yes			161.6
			3	408			no			299.4
			5	510			no			223.4

** La fn np.random.random() genera doubles aleatorios
** np.ramdom.randomint()	genera enteros

** En el paquete random -> random.randrange(0,100,7) desde a hasta , el 3er parametro puedo definir el multiplo

** np.arange(100) crea un array con el rango definido
** np.random.shuffle() mezcla el array

// Es una buena practica generar una semilla np.ramdom.seed()

***** FN DE DISTRIBUCION DE PROBABILIDADES (PDF)
	  formula f(x)=P(X=x)

		>>>>>Dentro de la distribucion de probabilidades existen 

	** DISTRIBUCION UNIFORME 
		formular f(x)= 1/b-a  basicamente es 1
 	** DISTRIBUCION NORMAL (GAUSS)
		formula X = mu (moda) + sd(distribucion standard) * Z (Normal standard)



** atributo.aggregate([np.sum, np.mean]) -> para agregar columnas
**atributo['Age'].filter(lambda x:x.sum()>2400) -> se puede filtrar por medio de fn lambda

 la fn transform y las lambdas son muy utiles para aplicar modificaciones

	por ejemplo  - score = lambda x : (x - x.mean()/x.std() )
				   grouped_gender.transform(score)


*** Conjunto de validacion / Testing

opc 1 -> dividir el dataset haciendo una distribuicion normal y luego separar en set de training y set Testing

		a = np.random.randn(len(data)) // genero normal de la cantidad del dataset
		check = (a < .8)               // divido el 80%
		training = data[check]
		testing = data[~check]

opc 2 -> Usuando SKLearn   
		from sklearn.model_selection import train_test_split  //importo el package

		train, test = train_test_split(data, test_size = 0.2) //divido el set

opc 3 -> Usando fn de shuffle de sklearn
		import sklearn.utils as sk // libreria utils

		data = sk.shuffle(data)
		cut_id = int(0.75 * len(data))  // asigno corte del %75 
		train_data = data[:cut_id]      // toma desde el 1er registro hasta el corte
		test_data = data[cut_id+1:]     // toma desde el corte +1 al final

*** Concatenar Dataframes

	pd.concat([dataset1, dataset2], axis=0  //filas)

*** carga de datos distribuidos / multiples datasets de carga local

	ejemplo 
			for i in range(2,333):
    			if i < 10:
        			filename = '00'+str(i)+'.csv'
    			elif i < 100:
            		filename + '0'+str(i)+'.csv'
    			else:
        			filename = str(i)+'.csv'

					file = filepath + filename

			tmp_data = pd.read_csv(file)
			data = pd.concat([data, tmp_data],axis =0)




